name: Scraper — Swiggy

on:
  push:
    branches: ["**"]  # suppresses phantom push-failure records; job is skipped on push
  schedule:
    - cron: "5 6 * * *"  # 11:35 AM IST daily
  workflow_dispatch:
    inputs:
      report_date:
        description: "Date to scrape (YYYY-MM-DD). Defaults to yesterday."
        required: false
        default: ""

jobs:
  run-scraper:
    name: Run Swiggy scraper
    runs-on: ubuntu-22.04
    timeout-minutes: 30
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          pip install -r scrapers/requirements.txt
          pip install pyotp google-auth google-auth-oauthlib google-api-python-client python-dotenv

      - name: Install Playwright browsers
        run: playwright install chromium --with-deps

      - name: Restore Google token
        run: echo "${{ secrets.GOOGLE_TOKEN_JSON }}" | base64 -d > token.json

      - name: Create data directories
        run: mkdir -p data/raw/swiggy

      - name: Run Swiggy scraper
        env:
          SWIGGY_LINK: ${{ secrets.SWIGGY_LINK }}
          SWIGGY_EMAIL: ${{ secrets.SWIGGY_EMAIL }}
          PROFILE_STORAGE_DRIVE_FOLDER_ID: ${{ secrets.PROFILE_STORAGE_DRIVE_FOLDER_ID }}
          GOOGLE_DRIVE_ROOT_FOLDER_ID: ${{ secrets.GOOGLE_DRIVE_ROOT_FOLDER_ID }}
          RAW_DATA_PATH: ./data/raw
          INPUT_REPORT_DATE: ${{ github.event.inputs.report_date }}
        run: |
          python - <<'EOF'
          import sys, os
          from datetime import date, timedelta
          sys.path.insert(0, '.')

          report_date_str = os.environ.get("INPUT_REPORT_DATE", "").strip()
          if report_date_str:
              from datetime import datetime
              report_date = datetime.strptime(report_date_str, "%Y-%m-%d").date()
          else:
              report_date = date.today() - timedelta(days=1)

          from scrapers.swiggy_scraper import SwiggyScraper
          result = SwiggyScraper(headless=True).run(report_date=report_date)
          print("Result:", result)
          if result.get("status") == "failed":
              sys.exit(1)
          EOF

      - name: Populate DB from downloaded data
        if: success()
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          POSTGRES_HOST: ${{ secrets.POSTGRES_HOST }}
          POSTGRES_PORT: ${{ secrets.POSTGRES_PORT }}
          POSTGRES_DB: ${{ secrets.POSTGRES_DB }}
          POSTGRES_USER: ${{ secrets.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          INPUT_REPORT_DATE: ${{ github.event.inputs.report_date }}
        run: |
          pip install -r backend/requirements.txt -q
          python - <<'EOF'
          import sys, os, re
          from datetime import date, timedelta
          from urllib.parse import urlparse, urlunparse
          sys.path.insert(0, '.')

          # Supabase direct DB (db.<ref>.supabase.co) is IPv6-only on GitHub Actions.
          # Rewrite to the connection pooler (<ref>.pooler.supabase.com) which has IPv4.
          def _pooler_url(url):
              if not url: return url
              _p = urlparse(url)
              _m = re.match(r'^(?:db\.)?([a-zA-Z0-9-]+)\.supabase\.co$', (_p.hostname or "").strip())
              if _m:
                  _ref = _m.group(1); _pooler = f"{_ref}.pooler.supabase.com"
                  print(f"[DB] Supabase pooler: {_p.hostname} -> {_pooler}", flush=True)
                  return urlunparse(_p._replace(netloc=_p.netloc.replace(_p.hostname, _pooler, 1)))
              return url
          os.environ["DATABASE_URL"] = _pooler_url(os.environ.get("DATABASE_URL", ""))
          _h = os.environ.get("POSTGRES_HOST", "").strip()
          _hparts = _h.split(".") if _h else []
          print(f"[DB-DIAG] HOST={len(_hparts)}pts,ends={'.'.join(_hparts[-2:]) if len(_hparts)>=2 else (_h or 'empty')},DB_URL={'set' if os.environ.get('DATABASE_URL') else 'empty'}", flush=True)
          report_date_str = os.environ.get("INPUT_REPORT_DATE", "").strip()
          if report_date_str:
              from datetime import datetime
              report_date = datetime.strptime(report_date_str, "%Y-%m-%d").date()
          else:
              report_date = date.today() - timedelta(days=1)

          from scrapers.orchestrator import populate_portal_data
          result = populate_portal_data(portal_name="swiggy", report_date=report_date)
          print("DB populate result:", result)
          EOF

      - name: Upload data artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: swiggy-data-${{ github.run_id }}
          path: data/raw/swiggy/
          retention-days: 7

      - name: Upload debug screenshots on failure
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: swiggy-debug-${{ github.run_id }}
          path: data/raw/swiggy/debug_swiggy_*.png
          retention-days: 3

      - name: Slack alert on failure
        if: failure()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          curl -s -X POST "$SLACK_WEBHOOK_URL" \
            -H "Content-Type: application/json" \
            -d '{
              "text": ":warning: *Swiggy scraper failed* — Gmail OTP token has likely expired.\n\n*Action needed:*\n1. Run locally: `python scrapers/auth_gmail.py` (re-authorize Gmail API)\n2. Base64-encode the new token: `base64 -w0 token.json`\n3. Update the *GOOGLE_TOKEN_JSON* GitHub secret with the new value\n4. Re-run the workflow: https://github.com/${{ github.repository }}/actions/workflows/scraper-swiggy.yml\n\n_Run:_ <https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}|#${{ github.run_id }}>"
            }'

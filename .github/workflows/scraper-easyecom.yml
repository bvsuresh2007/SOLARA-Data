name: Scraper — EasyEcom

on:
  push:
    branches: [main]  # suppresses phantom push-failure records; job is skipped on push
  schedule:
    - cron: "30 5 * * *"  # 11:00 AM IST daily
  workflow_dispatch:
    inputs:
      report_date:
        description: "Date to scrape (YYYY-MM-DD). Defaults to yesterday."
        required: false
        default: ""

jobs:
  run-scraper:
    name: Run EasyEcom scraper
    runs-on: ubuntu-latest
    timeout-minutes: 60
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          pip install -r scrapers/requirements.txt
          pip install pyotp google-auth google-auth-oauthlib google-api-python-client python-dotenv

      - name: Install Playwright browsers
        run: playwright install chromium --with-deps

      - name: Restore Google token
        run: echo "${{ secrets.GOOGLE_TOKEN_JSON }}" | base64 -d > token.json

      - name: Create data directories
        run: mkdir -p data/raw/easyecom

      - name: Run EasyEcom scraper
        env:
          POSTGRES_HOST: ${{ secrets.POSTGRES_HOST }}
          POSTGRES_PORT: ${{ secrets.POSTGRES_PORT }}
          POSTGRES_DB: ${{ secrets.POSTGRES_DB }}
          POSTGRES_USER: ${{ secrets.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          PROFILE_STORAGE_DRIVE_FOLDER_ID: ${{ secrets.PROFILE_STORAGE_DRIVE_FOLDER_ID }}
          RAW_DATA_PATH: ./data/raw
        run: |
          python - <<'EOF'
          import sys, os
          from datetime import date, timedelta
          sys.path.insert(0, '.')

          report_date_str = os.environ.get("INPUT_REPORT_DATE", "").strip()
          if report_date_str:
              from datetime import datetime
              report_date = datetime.strptime(report_date_str, "%Y-%m-%d").date()
          else:
              report_date = date.today() - timedelta(days=1)

          from scrapers.easyecom_scraper import EasyecomScraper
          result = EasyecomScraper(headless=True).run(report_date=report_date)
          print("Result:", result)
          if result.get("status") == "error":
              sys.exit(1)
          EOF
        env:
          INPUT_REPORT_DATE: ${{ github.event.inputs.report_date }}

      - name: Upload data artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: easyecom-data-${{ github.run_id }}
          path: data/raw/easyecom/
          retention-days: 7

      - name: Upload debug screenshots on failure
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: easyecom-debug-${{ github.run_id }}
          path: data/raw/easyecom/debug_*.png
          retention-days: 3

      - name: Slack alert — session expired
        if: failure()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          curl -s -X POST "$SLACK_WEBHOOK_URL" \
            -H "Content-Type: application/json" \
            -d '{
              "text": ":warning: *EasyEcom scraper failed* — Google OAuth session has likely expired.\n\n*Action needed:*\n1. Run locally in visible mode: `python scrapers/easyecom_scraper.py`\n2. Complete the Google login in the browser window\n3. Profile will auto-upload to Drive\n4. Re-run the workflow manually: https://github.com/${{ github.repository }}/actions/workflows/scraper-easyecom.yml\n\n_Run:_ <https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}|#${{ github.run_id }}>"
            }'

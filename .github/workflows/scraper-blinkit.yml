name: Scraper — Blinkit

on:
  push:
    branches: ["**"]  # suppresses phantom push-failure records; job is skipped on push
  schedule:
    - cron: "20 6 * * *"  # 11:50 AM IST daily
  workflow_dispatch:
    inputs:
      report_date:
        description: "Date to scrape (YYYY-MM-DD). Defaults to yesterday."
        required: false
        default: ""

jobs:
  run-scraper:
    name: Run Blinkit scraper
    runs-on: ubuntu-22.04
    timeout-minutes: 60
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          pip install -r scrapers/requirements.txt
          pip install pyotp google-auth google-auth-oauthlib google-api-python-client python-dotenv

      - name: Install Playwright browsers
        run: playwright install chromium --with-deps

      - name: Restore Google token
        run: echo "${{ secrets.GOOGLE_TOKEN_JSON }}" | base64 -d > token.json

      - name: Create data directories
        run: mkdir -p data/raw/blinkit

      - name: Check DB connection
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          # GitHub Actions ubuntu-22.04 prefers IPv6 but blocks outbound IPv6.
          # Force IPv4 so libpq uses A records for the Supabase pooler hostname.
          echo "precedence ::ffff:0:0/96  100" | sudo tee -a /etc/gai.conf
          pip install psycopg2-binary -q
          python - <<'EOF'
          import os, sys
          try:
              import psycopg2
              url = os.environ.get("DATABASE_URL", "")
              if not url:
                  print("DATABASE_URL not set — skipping pre-check")
                  sys.exit(0)
              conn = psycopg2.connect(url, connect_timeout=10)
              conn.close()
              print("DB connection: OK", flush=True)
          except Exception as e:
              print(f"DB connection FAILED: {e}", flush=True)
              sys.exit(1)
          EOF

      - name: Run Blinkit scraper
        env:
          BLINKIT_LINK: ${{ secrets.BLINKIT_LINK }}
          BLINKIT_EMAIL: ${{ secrets.BLINKIT_EMAIL }}
          POSTGRES_HOST: ${{ secrets.POSTGRES_HOST }}
          POSTGRES_PORT: ${{ secrets.POSTGRES_PORT }}
          POSTGRES_DB: ${{ secrets.POSTGRES_DB }}
          POSTGRES_USER: ${{ secrets.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          PROFILE_STORAGE_DRIVE_FOLDER_ID: ${{ secrets.PROFILE_STORAGE_DRIVE_FOLDER_ID }}
          GOOGLE_DRIVE_ROOT_FOLDER_ID: ${{ secrets.GOOGLE_DRIVE_ROOT_FOLDER_ID }}
          RAW_DATA_PATH: ./data/raw
          INPUT_REPORT_DATE: ${{ github.event.inputs.report_date }}
        run: |
          python - <<'EOF'
          import sys, os
          from datetime import date, timedelta
          sys.path.insert(0, '.')

          report_date_str = os.environ.get("INPUT_REPORT_DATE", "").strip()
          if report_date_str:
              from datetime import datetime
              report_date = datetime.strptime(report_date_str, "%Y-%m-%d").date()
          else:
              report_date = date.today() - timedelta(days=1)

          from scrapers.blinkit_scraper import BlinkitScraper
          result = BlinkitScraper(headless=True).run(report_date=report_date)
          print("Result:", result)
          if result.get("status") == "failed":
              sys.exit(1)
          EOF

      - name: Populate DB from downloaded data
        if: success()
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          POSTGRES_HOST: ${{ secrets.POSTGRES_HOST }}
          POSTGRES_PORT: ${{ secrets.POSTGRES_PORT }}
          POSTGRES_DB: ${{ secrets.POSTGRES_DB }}
          POSTGRES_USER: ${{ secrets.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          INPUT_REPORT_DATE: ${{ github.event.inputs.report_date }}
        run: |
          pip install -r backend/requirements.txt -q
          python - <<'EOF'
          import sys, os
          from datetime import date, timedelta
          sys.path.insert(0, '.')

          report_date_str = os.environ.get("INPUT_REPORT_DATE", "").strip()
          if report_date_str:
              from datetime import datetime
              report_date = datetime.strptime(report_date_str, "%Y-%m-%d").date()
          else:
              report_date = date.today() - timedelta(days=1)

          from scrapers.orchestrator import populate_portal_data
          result = populate_portal_data(portal_name="blinkit", report_date=report_date)
          print("DB populate result:", result)
          EOF

      - name: Upload data artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: blinkit-data-${{ github.run_id }}
          path: data/raw/blinkit/
          retention-days: 7

      - name: Upload debug screenshots on failure
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: blinkit-debug-${{ github.run_id }}
          path: data/raw/blinkit/debug_*.png
          retention-days: 3

      - name: Slack alert on failure
        if: failure()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          curl -s -X POST "$SLACK_WEBHOOK_URL" \
            -H "Content-Type: application/json" \
            -d '{
              "text": ":warning: *Blinkit scraper failed* — Gmail OTP token has likely expired.\n\n*Action needed:*\n1. Run locally: `python scrapers/auth_gmail.py` (re-authorize Gmail API)\n2. Base64-encode the new token: `base64 -w0 token.json`\n3. Update the *GOOGLE_TOKEN_JSON* GitHub secret with the new value\n4. Re-run the workflow: https://github.com/${{ github.repository }}/actions/workflows/scraper-blinkit.yml\n\n_Run:_ <https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}|#${{ github.run_id }}>"
            }'

# =============================================================================
# SolaraDashboard - Environment Variables Template
# Copy this file to .env and fill in your credentials
# =============================================================================

# --- Database Configuration ---
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=solara_dashboard
POSTGRES_USER=solara_user
POSTGRES_PASSWORD=your_secure_password_here

# --- Portal Credentials ---
SWIGGY_EMAIL=your_swiggy_email@example.com
SWIGGY_PASSWORD=your_swiggy_password

BLINKIT_EMAIL=your_blinkit_email@example.com
BLINKIT_PASSWORD=your_blinkit_password

AMAZON_EMAIL=your_amazon_email@example.com
AMAZON_PASSWORD=your_amazon_password

ZEPTO_EMAIL=your_zepto_email@example.com
ZEPTO_PASSWORD=your_zepto_password

SHOPIFY_API_KEY=your_shopify_api_key
SHOPIFY_API_SECRET=your_shopify_api_secret
SHOPIFY_STORE_URL=your-store.myshopify.com

MYNTRA_EMAIL=your_myntra_email@example.com
MYNTRA_PASSWORD=your_myntra_password

FLIPKART_EMAIL=your_flipkart_email@example.com
FLIPKART_PASSWORD=your_flipkart_password

# --- Slack Integration ---
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_BOT_TOKEN=xoxb-your-bot-token
SLACK_CHANNEL_ID=C0123456789

# --- Backend API ---
API_HOST=0.0.0.0
API_PORT=8000
API_SECRET_KEY=your_secret_key_for_jwt_change_this_in_production

# --- Frontend ---
NEXT_PUBLIC_API_URL=http://localhost:8000

# --- Scraper Configuration ---
# Schedule is interpreted in IST (Asia/Kolkata) because TZ is set in docker-compose.yml
SCRAPE_SCHEDULE_HOUR=11
SCRAPE_SCHEDULE_MINUTE=0
MAX_RETRY_ATTEMPTS=3
SCREENSHOT_ON_ERROR=true

# --- Data Storage ---
RAW_DATA_PATH=./data/raw
PROCESSED_DATA_PATH=./data/processed

# --- Google API (Gmail OTP + Drive upload) ---
# GMAIL_TOKEN_JSON: The full contents of token.json as a single-line JSON string.
#   Local dev: leave this unset — the code reads token.json from the project root instead.
#   CI / GitHub Actions: paste the contents of token.json here (or store as a GitHub Secret).
#   To generate token.json: run auth_gmail.py once, then copy its contents here.
GMAIL_TOKEN_JSON=

# GOOGLE_DRIVE_ROOT_FOLDER_ID: The folder ID of the "SolaraDashboard Reports" root folder
#   in Google Drive. Find it in the URL when you open the folder:
#   https://drive.google.com/drive/folders/<FOLDER_ID_HERE>
#   If left unset, the scraper will create/find a folder named "SolaraDashboard Reports"
#   in the root of My Drive automatically.
GOOGLE_DRIVE_ROOT_FOLDER_ID=

# --- Browser Profile Sync (Google Drive) ---
# PROFILE_STORAGE_DRIVE_FOLDER_ID: The ID of the "SolaraDashboard Profiles" folder in Google Drive.
#   This folder stores blinkit_profile.zip and easyecom_profile.zip so scrapers running
#   on CI (ephemeral runners) can download the authenticated profile before each run and
#   upload the refreshed profile after.
#
#   To create the folder and get the ID, run once:
#     python scripts/setup_profiles_drive_folder.py
#   Then paste the printed ID here.
#
#   Leave blank for local development — profile sync is silently skipped when unset.
PROFILE_STORAGE_DRIVE_FOLDER_ID=

# --- Amazon ASIN Tool (scrapers/tools/amazon_asin_scraper) ---
# No env vars needed; Slack webhook for ASIN tool is stored in slack_config.json
